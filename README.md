# 456FinalProject

Project Idea #1: Churn prediction

Churn prediction is the process of analyzing customer behavior to predict when a customer will stop using a product or service. It's a data-driven process that uses historical data, behavioral analytics, and machine learning algorithms to identify customers who are likely to churn.

About Dataset

Content

This data set is highly imbalanced and contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.


## General Data Preprocessing Steps (And Possible Things to Help)

- Clean Up Data: The process of analyzing and correcting errors or inconsistencies in the dataset. It involves handling missing values, removing duplicates, and correcting incorrect or outlier data to ensure the dataset is accurate and reliable.

- Data Integration: Involves combining data from multiple sources to create a unified dataset. This is often necessary when data is collected from different source systems. Some Techniques Include:
  - Record Linkage: The process of identifying and matching records from different datasets that refer to the same entity, even if they are represented differently. It helps in combining data from various sources by finding corresponding records based on common identifiers or attributes.
  - Data Fusion: Involves combining data from multiple sources to create a more comprehensive and accurate dataset. It integrates information that may be inconsistent or incomplete from different sources, ensuring a unified and richer dataset for analysis.
  - Schema Matching: Aligning fields and data structures from different sources to ensure consistency.
  - Data Deduplication: Identifying and removing duplicate entries across multiple datasets.
 
- Data Transformation: Involves converting data into a format suitable for analysis. Some techniques include:
  - Data Normalization: The process of scaling data to a common range to ensure consistency across variables.
  - Discretization: Converting continuous data into discrete categories for easier analysis.
  - Data Aggregation: Combining multiple data points into a summary form, such as averages or totals, to simplify analysis.
  - Concept Hierarchy Generation: Organizing data into a hierarchy of concepts to provide a higher-level view for better understanding and analysis.
  - Encoding categorical variables: Converting categorical data into numerical values using one-hot or label encoding techniques.
  - Feature engineering and extraction: Creating new features or selecting important ones to improve model performance.

- Data Reduction: Simplifies the dataset by reducing the number of features or records while preserving the essential information. This helps speed up analysis and model training without sacrificing accuracy.
  - Dimensionality Reduction (e.g., Principal Component Analysis): A technique that reduces the number of variables in a dataset while retaining its essential information.
  - Numerosity Reduction: Reducing the number of data points by methods like sampling to simplify the dataset without losing critical patterns.
  - Data Compression: Reducing the size of data by encoding it in a more compact form, making it easier to store and process.

Links: https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/ and https://www.datacamp.com/blog/data-preprocessing
